---
title: "DATA605 HW4"
author: "Misha Kollontai"
date: "2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Problem set 1

#### In this problem, we’ll verify using R that SVD and Eigenvalues are related as worked out in the weekly module. Given a 3 × 2 matrix **A**

$$\mathbf{A} = \left[\begin{array}
{rrr}
1 & 2 & 3 \\
-1 & 0 & 4 
\end{array}\right]$$

#### write code in R to compute $X = AA^{T}$ and $Y = A^{T}A$. Then, compute the eigenvalues and eigenvectors of X and Y using the built-in commands in R.

#### Then, compute the left-singular, singular values, and right-singular vectors of A using the $svd$ command. Examine the two sets of singular vectors and show that they are indeed eigenvectors of X and Y. In addition, the two non-zero eigenvalues (the 3rd value will be very close to zero, if not zero) of both X and Y are the same and are squares of the non-zero singular values of A.

#### Your code should compute all these vectors and scalars and store them in variables. Please add enough comments in your code to show me how to interpret your steps.


``` {r Prob1}

A <- matrix(c(1,-1,2,0,3,4), byrow = FALSE, nrow = 2)
#Calculate the transpose of A and assign to At
At <- t(A)

#Calculate A*At and assign to X
X <- A %*% At
#Calculate At*A and assign to Y
Y <- At %*% A

#Assign the eigenvalues/vectors of X and Y to 'evX' and 'evY' respectively
evX <- eigen(X)
evY <- eigen(Y)

#Assign the result of the 'svd' function to A_svd
A_svd <- svd(A)

print("Eigen decomposition of X is:")
evX
print("Eigen decomposition of Y is:")
evY
A_svd
```

**The two sets of singular vectors in A_svd match the eigenvectors caluclated for both X and Y.**

```{r}
#Display the eigenvalues of X (thirs eigenvalue on Y is near zero")
evX$values
#Display the square of the non-zero singular values of A.
A_svd$d ** 2

round(evX$values,6) == round(A_svd$d ** 2,6)
```

**The two sets of numbers match!**

## Problem Set 2

#### Using the procedure outlined in section 1 of the weekly handout, write a function to compute the inverse of a well-conditioned full-rank square matrix using co-factors. In order to compute the co-factors, you may use built-in commands to compute the determinant. Your function should have the following signature:

$$B = myinverse(A)$$

#### where **A** is a matrix and **B** is its inverse and $A*B=I$. The off-diagonal elements of **I** should be close to zero, if not zero. Likewise, the diagonal elements should be close to 1, if not 1. Small numerical precision errors are acceptable but the function myinverse should be correct and must use co-factors and determinant of **A** to compute the inverse.

```{r}
myinverse <- function(x){
  n <- nrow(x)
  m <- ncol(x)
  if(n!=m){
    print("The matrix provided is not square - cannot compute inverse!")
  } else {
    detA <- det(A)
    if (detA == 0){
      print("The determinant of A is zero, therefore A is non-invertible!")
    } else {
      C <- matrix(0L, nrow = n, ncol = m)
      for (i in 1:n){
        for (j in 1:m){
          C[i,j] <- det(A[-i,-j]) * (-1)^(i+j)
        }
      }
      Ct <- t(C)
      A_inv <- Ct/detA
      return(A_inv)
    }
  }
}
```

### Let's test this function on a non-invertible matrix (Sample matrix taken from https://en.wikipedia.org/wiki/Invertible_matrix)

```{r}
A <- matrix(c(-1,2/3,3/2,-1), byrow = FALSE, nrow = 2)

A

B <- myinverse(A)
```

### Let's test this function on a non-square matrix.

```{r}
A <- matrix(c(-6,8,7,9,3,-4,-9,1,16,5,13,-8), byrow = FALSE, nrow = 4)

A

B <- myinverse(A)
```

### Let's test this function on an arbitrary invertible 5x5 matrix..

```{r}
A <- matrix(c(9,10,12,19,23,6,14,15,2,24,8,4,16,20,3,13,5,17,21,25,7,11,18,22,26), byrow = FALSE, nrow = 5)

A

B <- myinverse(A)

B

round(A%*%B,10)
```

---

### As we can see, the product of A and B yields the identity matrix, confirming that B is the inverse of A.